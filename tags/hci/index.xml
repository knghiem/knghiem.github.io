<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hci on Khanh Nghiem</title>
    <link>https://knghiem.github.io/tags/hci/</link>
    <description>Recent content in hci on Khanh Nghiem</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 09 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://knghiem.github.io/tags/hci/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AI Coding Assistants - Looking Back and Forward</title>
      <link>https://knghiem.github.io/posts/ai-coding-assistants/</link>
      <pubDate>Fri, 09 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>https://knghiem.github.io/posts/ai-coding-assistants/</guid>
      <description>In the last two years, our AI community has learned a lot about large language models (LLMs) and how we could use them for human needs, including the need for building better, faster, and more reliable software. For much of this period, I was the product manager in a team that built AI coding assistants and closely followed the flow of ideas and discourse in this field. I want to compile and share in this blog post insights I compiled while working in the AI field, regarding building the next-generation AI coding assistant across four main themes:</description>
    </item>
    
  </channel>
</rss>
